import subprocess
import time
import os
import logging
import json
import sys

# --- Configuration ---
# GitHub repository details automatically detected
GITHUB_OWNER = "technoplato"
GITHUB_REPO = "zmk-config"
# TODO: Adjust artifact names if they differ from the ZMK defaults for glove80
# Common examples: "glove80_left-seeeduino_xiao_ble-zmk.uf2", "glove80_right-seeeduino_xiao_ble-zmk.uf2"
# Or: "glove80_left-nice_nano_v2-zmk.uf2", "glove80_right-nice_nano_v2-zmk.uf2"
# --- Artifact Configuration ---
# Name of the artifact generated by GitHub Actions (often a zip file)
ARTIFACT_NAME = "firmware"
# Expected firmware files *inside* the artifact (adjust if needed)
EXPECTED_FILES_IN_ARTIFACT = ["glove80_left-glove80-zmk.uf2", "glove80_right-glove80-zmk.uf2"]
# Directory to save downloaded firmware relative to the script's location
DOWNLOAD_DIR = "firmware"
# Polling interval in seconds to check workflow status
POLL_INTERVAL = 15 # Check every 15 seconds to avoid hitting API rate limits
# Git commit message
COMMIT_MESSAGE = "Update ZMK configuration via script"
# Log file name (will be created in the same directory as the script)
LOG_FILE = "update_firmware.log"

# --- Logging Setup ---
# Get the directory where the script is located
script_dir = os.path.dirname(os.path.abspath(__file__))
log_file_path = os.path.join(script_dir, LOG_FILE)
download_dir_path = os.path.join(script_dir, DOWNLOAD_DIR) # Download relative to script

log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
# File Handler
log_handler = logging.FileHandler(log_file_path)
log_handler.setFormatter(log_formatter)
# Console Handler
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(log_formatter)
# Logger setup
logger = logging.getLogger()
logger.setLevel(logging.INFO)
# Clear existing handlers if any (useful for re-runs in some environments)
if logger.hasHandlers():
    logger.handlers.clear()
logger.addHandler(log_handler)
logger.addHandler(console_handler)

# --- Helper Functions ---
def run_command(command, check=True, capture_output=False, text=True, shell=False, cwd=None):
    """Runs a shell command and logs its output."""
    command_str = ' '.join(command) if isinstance(command, list) else command
    logger.info(f"Running command: {command_str} (in {cwd or 'default CWD'})")
    try:
        result = subprocess.run(
            command,
            check=check,
            capture_output=capture_output,
            text=text,
            shell=shell, # Use shell=True carefully
            cwd=cwd      # Set working directory if provided
        )
        if capture_output:
            stdout = result.stdout.strip() if result.stdout else ""
            stderr = result.stderr.strip() if result.stderr else ""
            if stdout:
                logger.debug(f"Command stdout: {stdout}")
            if stderr:
                 logger.debug(f"Command stderr: {stderr}") # Use debug for stderr too unless error
        return result
    except subprocess.CalledProcessError as e:
        logger.error(f"Command failed: {command_str}")
        logger.error(f"Error code: {e.returncode}")
        # Log stdout/stderr from the exception object if available
        stdout = e.stdout.strip() if e.stdout else ""
        stderr = e.stderr.strip() if e.stderr else ""
        if stdout:
            logger.error(f"Stdout: {stdout}")
        if stderr:
            logger.error(f"Stderr: {stderr}")
        raise
    except FileNotFoundError:
        cmd_name = command[0] if isinstance(command, list) else command.split()[0]
        logger.error(f"Error: Command not found: {cmd_name}. Is it installed and in PATH?")
        raise

def git_push_changes(repo_path):
    """Stages, commits, and pushes changes to the repository."""
    logger.info("Checking for local changes...")
    # Check git status
    status_result = run_command(["git", "status", "--porcelain"], capture_output=True, cwd=repo_path)
    if not status_result.stdout.strip():
        logger.info("No changes detected. Skipping git operations.")
        # Get the SHA of the last commit to find the relevant workflow run
        head_sha_result = run_command(["git", "rev-parse", "HEAD"], capture_output=True, cwd=repo_path)
        return head_sha_result.stdout.strip()

    logger.info("Staging changes...")
    run_command(["git", "add", "."], cwd=repo_path)

    logger.info(f"Committing changes with message: '{COMMIT_MESSAGE}'")
    # Allow empty commit in case only unstaged changes were added, though `git add .` should handle it.
    # Use --allow-empty? No, better to check status first. If status is clean, we already returned.
    run_command(["git", "commit", "-m", COMMIT_MESSAGE], cwd=repo_path)

    logger.info("Pushing changes to remote repository...")
    # Capture output to potentially get commit SHA, though rev-parse is more reliable
    run_command(["git", "push"], capture_output=True, cwd=repo_path)

    # Get the pushed commit SHA reliably
    head_sha_result = run_command(["git", "rev-parse", "HEAD"], capture_output=True, cwd=repo_path)
    pushed_sha = head_sha_result.stdout.strip()
    logger.info(f"Successfully pushed commit: {pushed_sha}")
    return pushed_sha


def get_latest_workflow_run_id(commit_sha):
    """Gets the ID of the latest workflow run associated with a specific commit SHA."""
    logger.info(f"Fetching workflow runs for commit: {commit_sha}...")
    # Command to list workflow runs, filtering by the triggering commit SHA
    # Requires gh cli v2.5.0+ for --json fields
    # We look for runs triggered by 'push' or 'workflow_dispatch' matching the commit
    command = [
        "gh", "run", "list",
        "--repo", f"{GITHUB_OWNER}/{GITHUB_REPO}",
        "--json", "id,headSha,status,conclusion,event",
        "--limit", "10" # Check the 10 most recent runs
    ]

    attempts = 10 # Try for a while, actions might take a moment to register
    for attempt in range(attempts):
        logger.info(f"Attempt {attempt + 1}/{attempts} to find workflow run for commit {commit_sha}...")
        try:
            result = run_command(command, capture_output=True)
            runs = json.loads(result.stdout)
            logger.debug(f"Found {len(runs)} recent runs from API.")

            # Filter runs matching the commit SHA and triggered by relevant events
            relevant_runs = [
                run for run in runs
                if run['headSha'] == commit_sha and run['event'] in ['push', 'workflow_dispatch']
            ]

            if relevant_runs:
                # Runs are listed newest first by 'gh run list'
                latest_run = relevant_runs[0]
                run_id = latest_run['id']
                logger.info(f"Found latest workflow run ID: {run_id} for commit {commit_sha}")
                return run_id
            else:
                logger.info(f"No workflow run found yet for commit {commit_sha}. Waiting {POLL_INTERVAL} seconds...")

        except json.JSONDecodeError:
            logger.error(f"Failed to parse JSON output from 'gh run list'. Output: {result.stdout}")
            # Continue retrying
        except subprocess.CalledProcessError:
            logger.error("Failed to execute 'gh run list'. Check gh CLI authentication and permissions.")
            # Continue retrying, maybe it's a transient issue
        except Exception as e:
            logger.error(f"An unexpected error occurred while fetching workflow runs: {e}")
            # Continue retrying

        time.sleep(POLL_INTERVAL)

    logger.error(f"Could not find workflow run for commit {commit_sha} after {attempts} attempts.")
    return None


def wait_for_workflow_completion(run_id):
    """Polls the workflow run status until it completes."""
    logger.info(f"Waiting for workflow run {run_id} to complete...")
    while True:
        command = [
            "gh", "run", "view", str(run_id),
            "--repo", f"{GITHUB_OWNER}/{GITHUB_REPO}",
            "--json", "status,conclusion"
        ]
        try:
            result = run_command(command, capture_output=True)
            run_data = json.loads(result.stdout)
            status = run_data.get('status')
            conclusion = run_data.get('conclusion') # Will be None if not completed

            logger.info(f"Workflow run {run_id} status: {status}, Conclusion: {conclusion or 'N/A'}")

            if status == "completed":
                if conclusion == "success":
                    logger.info(f"Workflow run {run_id} completed successfully.")
                    return True
                else:
                    # Includes failure, cancelled, skipped, timed_out
                    logger.error(f"Workflow run {run_id} completed with non-success conclusion: {conclusion}")
                    return False
            elif status in ["cancelled", "failure", "skipped", "timed_out"]:
                 # Should be caught by conclusion='success' check, but belt-and-suspenders
                 logger.error(f"Workflow run {run_id} reported status: {status}. Conclusion: {conclusion}")
                 return False

            # If status is queued, in_progress, waiting, requested, etc.
            logger.info(f"Workflow still running. Checking again in {POLL_INTERVAL} seconds...")
            time.sleep(POLL_INTERVAL)

        except json.JSONDecodeError:
            logger.error(f"Failed to parse JSON output from 'gh run view {run_id}'. Output: {result.stdout}")
            logger.info(f"Retrying status check in {POLL_INTERVAL} seconds...")
            time.sleep(POLL_INTERVAL)
        except subprocess.CalledProcessError:
            # Maybe the run ID was wrong or transient API issue
            logger.error(f"Failed to execute 'gh run view {run_id}'. Check run ID and permissions.")
            logger.info(f"Retrying status check in {POLL_INTERVAL} seconds...")
            time.sleep(POLL_INTERVAL)
        except Exception as e:
            logger.error(f"An unexpected error occurred while checking workflow status: {e}")
            logger.info(f"Retrying status check in {POLL_INTERVAL} seconds...")
            time.sleep(POLL_INTERVAL)


def download_artifacts(run_id):
    """Downloads the specified artifacts from the completed workflow run."""
    logger.info(f"Attempting to download artifacts for run {run_id} to {download_dir_path}...")

    # Create download directory if it doesn't exist
    try:
        if not os.path.exists(download_dir_path):
            logger.info(f"Creating download directory: {download_dir_path}")
            os.makedirs(download_dir_path)
        elif not os.path.isdir(download_dir_path):
            logger.error(f"Error: Path '{download_dir_path}' exists but is not a directory.")
            return False
    except OSError as e:
        logger.error(f"Error creating directory {download_dir_path}: {e}")
        return False

    # Download the single artifact (gh cli handles extraction if it's a zip)
    logger.info(f"Downloading artifact: {ARTIFACT_NAME}...")
    command = [
        "gh", "run", "download", str(run_id),
        "--repo", f"{GITHUB_OWNER}/{GITHUB_REPO}",
        "--name", ARTIFACT_NAME,
        "--dir", download_dir_path # Download *into* this directory
    ]
    try:
        run_command(command)
        logger.info(f"Successfully executed download command for artifact '{ARTIFACT_NAME}'.")

        # --- Verification ---
        # Check if the expected files exist within the download directory after extraction
        missing_files = []
        found_files_count = 0
        for expected_file in EXPECTED_FILES_IN_ARTIFACT:
            expected_path = os.path.join(download_dir_path, expected_file)
            if os.path.exists(expected_path):
                logger.info(f"Verified: Found expected file '{expected_file}' at '{expected_path}'")
                found_files_count += 1
            else:
                logger.warning(f"Verification failed: Expected file '{expected_file}' not found at '{expected_path}'")
                missing_files.append(expected_file)

        # Log contents of download dir for debugging if files are missing
        if missing_files:
             try:
                 dir_contents = os.listdir(download_dir_path)
                 logger.debug(f"Contents of download directory '{download_dir_path}': {dir_contents}")
             except OSError as list_e:
                 logger.debug(f"Could not list contents of download directory '{download_dir_path}': {list_e}")

        # --- Final Check ---
        if found_files_count == len(EXPECTED_FILES_IN_ARTIFACT):
            logger.info(f"All {len(EXPECTED_FILES_IN_ARTIFACT)} expected firmware files verified in '{download_dir_path}'.")
            return True
        else:
            logger.error(f"Failed to verify all expected firmware files. Missing: {missing_files}")
            return False

    except subprocess.CalledProcessError:
        # This often means the artifact name wasn't found for that run
        logger.error(f"Failed to download artifact: {ARTIFACT_NAME}. Check artifact name and if it was generated in run {run_id}.")
        return False
    except Exception as e:
        logger.error(f"An unexpected error occurred during download of {ARTIFACT_NAME}: {e}")
        return False

# --- Main Execution ---
if __name__ == "__main__":
    logger.info("--- Starting Firmware Update Script ---")

    # Determine the repository root directory (assuming script is in 'scripts' subdir)
    repo_root = os.path.abspath(os.path.join(script_dir, '..'))
    logger.info(f"Operating on repository root: {repo_root}")

    # Basic check for gh CLI
    try:
        run_command(["gh", "--version"], capture_output=True)
        logger.info("GitHub CLI found.")
    except (FileNotFoundError, subprocess.CalledProcessError):
        logger.critical("GitHub CLI ('gh') not found or not working. Please install and authenticate it (gh auth login).")
        sys.exit(1)

    # Check for placeholder config
    if GITHUB_OWNER == "your_github_username" or GITHUB_REPO == "your_zmk_config_repo_name":
        logger.critical("Please update GITHUB_OWNER and GITHUB_REPO variables at the top of the script.") # Should not happen now
        sys.exit(1)
    if "glove80-zmk.uf2" in "".join(EXPECTED_FILES_IN_ARTIFACT): # Basic check for default placeholder
        logger.warning("Default EXPECTED_FILES_IN_ARTIFACT detected. Please verify these match the actual firmware file names *inside* the 'firmware' artifact.")


    try:
        # 1. Push changes (if any) and get commit SHA
        # Run git commands from the repository root
        commit_sha = git_push_changes(repo_path=repo_root)
        if not commit_sha:
             logger.error("Failed to get commit SHA. Exiting.")
             sys.exit(1)
        logger.info(f"Using commit SHA: {commit_sha} to find workflow run.")

        # 2. Get the latest workflow run ID for that commit
        run_id = get_latest_workflow_run_id(commit_sha)
        if not run_id:
            logger.error("Failed to find a relevant workflow run after multiple attempts. Check GitHub Actions for the commit. Exiting.")
            sys.exit(1)

        # 3. Wait for the workflow to complete successfully
        if not wait_for_workflow_completion(run_id):
            logger.error("Workflow did not complete successfully. Check the run logs on GitHub. Exiting.")
            sys.exit(1)

        # 4. Download the artifacts
        if not download_artifacts(run_id):
            logger.error("Failed to download all firmware artifacts. Check the run logs and artifact names. Exiting.")
            sys.exit(1)

        logger.info("--- Firmware Update Script Finished Successfully ---")
        sys.exit(0)

    except subprocess.CalledProcessError:
        logger.critical("A critical command failed unexpectedly. Check logs above for details. Exiting.")
        sys.exit(1)
    except FileNotFoundError:
         logger.critical("A required command (like git or gh) was not found. Ensure they are installed and in your PATH. Exiting.")
         sys.exit(1)
    except Exception as e:
        logger.critical(f"An unexpected script error occurred: {e}", exc_info=True) # Log traceback
        sys.exit(1)